# tcc

Ferramenta de Predição de Ação de Usuários e de Clusterização de Usuários  em Perfis de acordo com comportamento em Jogos Mobile

INTRODUÇÃO

	No campo de análise de dados, um dos maiores problemas enfrentados por analistas é a dificuldade de predizer ações futuras advindas de usuários. Análises de dados convencionais permitem geração de compreensões, hipóteses e soluções de problemas de modo a auxiliar tomadas de decisão em empresas, porém muitas vezes é incapaz de encontrar correlações mais complexas entre os dados, sendo, assim, fortemente apoiada por métodos automatizados de análise de dados. Por meio do uso de métodos de aprendizado de máquina, é possível encontrar correlações entre os dados de forma mais eficiente, segura, precisa e não tendenciosa, por não possuir possíveis opiniões humanas nas análises. Neste sentido, análise de dados e aprendizado de máquina possuem forte intertextualidade, sendo, em suma, a fonte do tema deste projeto. 
No mercado de trabalho, o trabalho de um analista de dados consiste da criação, análise e acompanhamento de métricas de interesse de performance dos jogos e da empresa como um todo, os chamados Indicadores-Chave de Desempenho (Key Performance Indicators - KPIs). Este trabalho baseia-se na observação de alterações nos valores das KPIs devido alterações dos ambientes aos quais os jogos, a empresa e os jogadores são expostos. Com isso, o trabalho de um analista de dados consiste majoritariamente da observação e acompanhamento de métricas e, a partir disso, levantamento de hipóteses, muitas vezes difíceis de comprovar, e por vezes de um modo pouco automatizado, incompleto e demorado. A construção de ferramentas que realizem predição de ações de usuários em jogos mobile permite efetuar análises mais complexas e aprofundadas, dentre elas prever se um usuário continuará a jogar no dia seguinte ou não, se a experiência dele em uma determinada etapa do jogo o fará desistir de sua experiência no jogo, se as ações performadas pelo usuário até o momento indicam comportamento de um futuro pagante, entre outras.
Assim, a motivação do desenvolvimento deste projeto é possibilitar não apenas automatizar os processos de análise de dados em jogos mobile, como também possibilitar análises mais profundas e preditivas sobre o comportamento e as ações dos usuários, que não são possíveis sem modelos estatísticos apropriados e bem estruturados. Com isso, não apenas torna as tarefas de um analista de dados mais rápidas e eficientes, como também possibilita novos tipos de análises, antes inexploráveis sem as devidas ferramentas e modelos.
	Sob o prisma da análise de dados aplicada a jogos mobile, definem-se eventos como ações tomadas por usuários dentro do jogo analisado, podendo ser desde a abertura de uma tela, efetuação de login, minimização do aplicativo, até a interação com algum objeto ou funcionalidade dentro do jogo. Tais eventos são configurados de forma a apresentarem informações pertinentes para sua análise, possuindo tanto características de usuários, como identificação (ID), país, cidade, modelo de celular, quanto características do evento, como horário, tela na qual ocorreu o evento, funcionalidade do jogo com a qual houve interação, e quaisquer outras que sejam desejadas.
	Dessa forma, com base nestes eventos, armazenados em bancos de dados estruturados, têm-se informações básicas sobre os usuários, como país e modelo de celular, e informações mais aprofundadas e detalhadas sobre os eventos efetuados por eles. Tais informações são úteis para alimentar os métodos e a ferramenta como um todo, de forma a buscar correlações entre os dados e eventos.
	Em suma, o desenvolvimento desta ferramenta permite melhor qualidade na análise e, com isso, possibilita analistas a melhorar a experiência dos usuários com a plataforma, melhorando, ultimamente, o divertimento dos usuários com os jogos. Além disso, tendo em vista que análises convencionais não levam em consideração diferenças entre possíveis perfis de comportamento dentre os usuários, agrupando-os da mesma forma nas análises. A construção de uma ferramenta de clusterização de usuários de acordo com perfis definidos por seu comportamento permite verificar o comportamento das KPIs separadamente entre os diferentes grupos de usuários. 
Neste sentido, o objetivo deste projeto é desenvolver uma ferramenta capaz de predizer ação de usuários, e clusterizá-los e classificá-los de acordo com seu perfil de comportamento dentro dos jogos.

ESPECIFICAÇÃO 

O sistema tem como funcionalidades: 
Construir funções de mapeamento de comportamento de usuário, que terão como objetivo predizer futuras ações de usuários com base nos dados passados coletados, e verificar o impacto do uso apenas de variáveis a partir de certo valor de correlação de distância;
Encontrar classificação de usuários, clusterizando-os em perfis de acordo com seu comportamento;

ASPECTOS CONCEITUAIS

Correlação
	
O primeiro objetivo do projeto é identificar se a correlação entre variáveis é relevante ao efetuar a predição de uma em relação às outras. Correlação é definida como o estudo do grau de associação entre duas variáveis. O coeficiente de correlação é uma medida padronizada entre duas variáveis, cujo objetivo é indicar a força e a direção do relacionamento entre essas duas variáveis aleatórias, ou seja, fornecer informações sobre o tipo e a extensão do relacionamento entre duas variáveis X e Y. 
	Embora seja comumente denotada como a medida de relação entre duas variáveis aleatórias, é de alta importância ressaltar que correlação não implica causalidade, assim como, em alguns casos, correlação não identifica dependência entre as variáveis, ou seja, existem casos nos quais as variáveis apresentam forte dependência estatística, porém possuem correlação nula.
O valor de um coeficiente de correlação pode ser positivo ou negativo, porém nunca maior do que um ou menor do que menos um. Um valor próximo a zero indica que as duas variáveis não estão relacionadas. Da mesma forma, valores próximos a um indicam que as duas variáveis são fortemente relacionadas na mesma direção, e valores próximos a menos um indicam que as duas variáveis são fortemente relacionadas em direções opostas. Além disso, uma correlação pode ser linear, na qual é possível ajustar uma reta entre as correlações, e a proximidade entre tal reta e as observações determina a força da correlação, ou uma correlação pode ser não linear, na qual não é possível ajustar uma reta entre as observações.
Dessa forma, para cumprir o objetivo do projeto, foi necessário investigar qualquer tipo de correlação entre as variáveis desejadas, não somente a correlação linear. Para tal, foi escolhida a correlação de distância, uma medida de dependência estatística entre duas variáveis ou vetores, que possui valor entre 0 e 1, 0 indicando que as variáveis são independentes, e 1 indicando que as variáveis são dependentes, mas não necessariamente linearmente, ao contrário de, por exemplo, a correlação de Pearson, e também não necessariamente seguindo alguma outra relação matemática mais usual, como, por exemplo, correlação quadrática ou correlação exponencial. Por fim, a escolha da medida de correlação de distância para identificar e mensurar a relação entre as variáveis foi fortemente suportada por sua provada consistência matemática e estatística. A partir de tal medida, verificou-se o impacto da escolha de variáveis com diferentes graus de correlação de distância, abordado em tópicos a seguir.	
	Primeiramente, é necessário definir matematicamente o que é a correlação de distância de uma amostra. Seja (Xk, Yk) , k = 1, ... , n uma amostra estatística de um vetor de variáveis numéricas aleatórias (X, Y). Computa-se a matriz de distância n por n (aj, k) e (bj, k) contendo os valores par a par de distância Euclidiana, como mostrado abaixo:

	Em seguida, selecionam-se todas as distâncias duplamente centradas, onde aj é a média da j-ésima linha e ak é a média da k-ésima coluna, e a.. é a média das médias da matriz de distância da amostra X. O raciocínio é o mesmo para os valores de b. 

	O quadrado da covariância de distância da amostra é dado pela média aritmética dos produtos Aj,k e Bj,k, como ilustrado abaixo:

	Por fim, a correlação de distância entre duas variáveis aleatórias é obtida por meio da divisão de sua covariância de distância pelo produto dos desvios padrões de distância, como ilustrado abaixo:

	

	Predição de Ação de Usuários

De forma a predizer os valores das métricas de interesse, e, com isso, predizer ações de usuários escolheu-se utilizar de métodos supervisionados de machine learning, ideais para quando deseja-se descobrir uma função mapeadora de uma entrada para uma saída, de forma a aproximá-la tanto que, quando houver novas entradas, será possível prever suas saídas a partir dos resultados das medições anteriores. Estes métodos podem ser de classificação, quando a saída da função é uma categoria, como “vermelho”, “azul”, ou podem ser de regressão, quando a saída da função é um valor numérico, como “dólares”, “gramas”, entre outros. Com estes, podem ser mapeados valores específicos desejados como, por exemplo, uma estimativa de quanto os usuários gastarão no jogo no total, quantas sessões por dia efetuarão, seu tempo médio de sessão, entre outros. 
Métodos supervisionados são algoritmos que fazem previsões com base em um conjunto de exemplos, armazenados nos dados, de forma a chegar no resultado desejado. Estes métodos supervisionados podem pertencer a três categorias, dependendo do tipo de análise desejada, podendo ser elas: Classificação, quando os dados estiverem sendo usados para prever uma categoria (Exemplo: Apresentar uma imagem como sendo uma moto, um carro ou um caminhão), Regressão, quando um valor estiver sendo previsto (Exemplo: preço de cotações), ou de Detecção de Anomalias, quando o objetivo for identificar pontos de dados incomuns (Exemplo: Fraudes em cartão de crédito).
Primeiramente, deve-se estudar e analisar quais dos métodos supervisionados de aprendizado de máquina podem ser apropriados para a solução do problema proposto. O primeiro método analisado foi o método de regressão logística. Em estatística, regressão logística é um modelo no qual a variável dependente, ou seja, a variável a ser predita, é categórica, e tem por objetivo medir a relação entre a variável categórica a ser predita e uma ou mais variáveis independentes por meio da estimativa de probabilidades fazendo uso de uma função logística, que consiste de uma função sigmóide.
Com isso, trata-se de um modelo de regressão útil para modelar a probabilidade de um evento ocorrer, em função de outros fatores, com implementações eficientes e de alta usabilidade. Entretanto, regressão logística depende de transformações não-lineares das variáveis preditoras. Além disso, não performa bem quando o espaço das funções é muito grande, nem com alto número de variáveis preditoras, o que o torna inadequado como solução ao problema proposto.
O segundo algoritmo analisado foi a máquina de vetores de suporte (support vector machine - SVM). As máquinas de vetores de suporte constituem uma técnica embasada na teoria de aprendizado estático, que estabelece uma série de princípios que devem ser seguidos na obtenção de classificadores com boa generalização, definida como a sua capacidade de prever corretamente a classe de novos dados do mesmo domínio em que o aprendizado ocorreu. As máquinas de vetores de suporte fazem uso dos dados de fronteira para construir a curva de separação para classificação. Com isso, lidam bem com fronteiras não lineares, além de lidarem bem com bases de dados de grande número de dimensões. Entretanto, não lidam bem com bases de dados com grande número de observações, o que o torna inutilizável como solução ao problema proposto.
 Por fim, foi analisado o algoritmo de Random Forest. Random Forest é um tipo de “ensemble learning”, ou seja, um método que gera muitos classificadores e combina seus resultados. Mais especificamente, o Random Forest gera diversas árvores de decisão, cada uma com suas particularidades, construídas em cima de porções diferentes e aleatórias de dados, e combina o resultado da classificação de todas elas. As árvores de decisão são construídas a partir dos diferentes valores das variáveis preditivas escolhidas e, a partir delas, decidir o valor da variável a ser predita.
Sob esta ótica, o método supervisionado de machine learning Random Forest, ou Random Decision Trees, consiste de um método de classificação e regressão, que opera por meio da construção de grande número de árvores de decisão, cada uma sob uma fração aleatória dos dados, o que resolve o problema de “overfitting”(a produção de uma análise que corresponde muito de perto ou exatamente a um determinado conjunto de dados e, portanto, pode deixar de se ajustar a dados adicionais ou prever as observações futuras de forma confiável).
Alguns dos maiores pontos fortes do algoritmo de Random Forest são:
Rápido tempo de execução, incluindo tempo de treino;
Capacidade de lidar com dados desbalanceados;
Capacidade de lidar com dados faltantes;
Pouca preparação prévia dos dados, ou seja, o modelo é capaz de lidar com dados binários, categóricos e numéricos;
Performa feature selection implicitamente, além de prover um ótimo indicador de feature importance;
Alta acurácia e precisão;
Alta versatilidade e simplicidade de uso;
Aumento do número de árvores do modelo não leva a overfit;
Automaticamente performa cross-validation;
Permite seleção de um limite no qual classificam-se os dados com maior precisão, de forma a, por exemplo, classificá-los somente se estiverem dentro do limite estabelecido;
Por outro lado, existe o ponto negativo de que, quando usado para regressão, o algoritmo não é capaz de predizer valores além dos existentes na base de treino. Além disso, o tamanho do modelo pode levar facilmente a centenas de megabytes de memória. Como para o problema proposto deseja-se executar uma classificação de forma a prever a ação dos usuários, baseado no comportamento dos dias anteriores, e como a limitação de tamanho de memória não é, de fato, um problema, o algoritmo de random forest constitui a melhor solução encontrada ao problema proposto.

	Clusterização de Usuários de acordo com Perfil de Comportamento

Primeiramente, explica-se o que é clusterização. Clusterização é uma análise de agrupamentos, ou seja, uma análise na qual constroem-se clusters, coleções de objetos similares entre si, e dissimilares aos objetos de outros clusters. Neste sentido, clusterização é um método não supervisionado de machine learning, visto que não possui classes predefinidas. A clusterização é usada para reconhecimento de padrões, análise de dados espaciais, classificação, agrupamento de usuários, organização de um conjunto de padrões,  entre outros, em clusters, de acordo com padrões de alguma medida de similaridade. Dessa forma, padrões pertencentes a um cluster devem ser mais “similares” entre si do que em relação a padrões pertencentes a outros clusters. Ou seja, o problema é agrupar os dados de acordo com um conjunto de padrões não rotulados em clusters de tal modo que os padrões apresentem alguma propriedade comum. 
O conceito de clusterização em sí está normalmente associado à análise exploratória, pois envolve problemas nos quais há pouca informação a priori acerca dos dados e poucas hipóteses a serem sustentadas. Dessa forma, é justamente a clusterização que auxilia no fornecimento de novas hipóteses a respeito dos inter-relacionamentos dos dados e de sua estrutura intrínseca.
Em geral, a atividade de clusterização envolve:
Representação de padrões, que, por sua vez, envolve definição do número, tipo e modo de apresentação dos atributos que descrevem cada padrão;
Seleção de características, ou seja, processo de identificação do subconjunto hipoteticamente mais efetivo dos atributos disponíveis para descrever cada padrão; 
Extração de características, que consiste de transformações dos atributos de entrada de forma a salientar uma ou mais características dentre aquelas que, hipoteticamente, estão presentes nos dados;
Definição de uma medida de similaridade apropriada ao domínio da aplicação (no caso, densidade e proximidade dimensional), por meio da definição de uma função de distância entre pares de dados, o que pode incluir aspectos conceituais e/ou numéricos (ou seja, qualitativos e/ou quantitativos);
Agrupamentos de dados, que pode ser hierárquico, ou seja, com um processo recursivo de junções ou separações de grupos, ou não-hierárquico, com o emprego de técnicas de discriminação de clusters;
 Apresentação de resultados, o que implica permitir a visualização gráfica dos clusters e a compreensão de suas inter-relações, por meio da proposição de protótipos ou outras descrições compactas para os clusters.
No caso de clusterização em bases de dados com muitas dimensões, há, basicamente, quatro problemas que devem ser resolvidos: 
Múltiplas dimensões são impossíveis, para a compreensão humana, de visualizar. Além disso, devido ao crescimento exponencial do número de possibilidades com o acréscimo de cada dimensão, enumeração completa de todos os subespaços torna-se intratável com o aumento da dimensionalidade;
O conceito de distância torna-se menos preciso com o aumento da dimensionalidade, visto que a distância entre dois pontos quaisquer em um dataset de muitas dimensões converge com o aumento da dimensionalidade;
Considerando que um cluster tem por objetivo caracterizar um grupo de objetos similares, baseado na observação de seus atributos, o aumento da dimensionalidade faz com que alguns destes atributos se tornem menos significativos para clusterização;
A alta dimensionalidade aumenta a chance de seus atributos apresentarem alta correlação;
Para enfrentar os problemas apontados acima, foi escolhida a abordagem de reduzir a dimensionalidade dos dados. Para tal, será usada a técnica de de Análise de Componentes Principais (Principal Component Analysis - PCA), que consiste de um procedimento matemático que utiliza transformação ortogonal para converter um conjunto de observações de variáveis possivelmente correlacionadas num conjunto de variáveis linearmente não correlacionadas chamadas de componentes principais. Matematicamente, o PCA é definido como uma transformação linear ortogonal, que transforma os dados para um novo sistema de coordenadas de forma que a maior variância por qualquer projeção dos dados fica ao longo da primeira coordenada (primeira componente principal), a segunda maior variância fica ao longo da segunda coordenada (segunda componente principal), e assim por diante. Neste sentido, as componentes principais são combinações lineares das variáveis originais.
A partir do descobrimento das componentes principais, descobre-se, também, quais das variáveis originais causam maior variância nos dados. Estas variáveis são as de maior importância para clusterização e, portanto, serão as variáveis selecionadas.
Com as dificuldades da alta dimensionalidade contornadas, é necessário escolher uma abordagem apropriada para a clusterização em si. Para tal, é importante abordar da melhor forma possível as práticas mais efetivas e consolidadas em clusterização em análise exploratória de dados. Dentre elas, encontram-se:
Não atingir resultado algum é melhor do que atingir um resultado errado. Resultados errôneos levam à intuições erradas. Ou seja, não entender os dados é melhor que entendê-los errado. Isso significa que um bom algoritmo de clusterização, quando aplicado em análises exploratórias de dados, deve ser capaz de não agrupar pontos que realmente não devem ser agrupados, ou seja, deve ser capaz de não classificar pontos como pertencentes à cluster algum;
Os parâmetros de clusterização devem ser intuitivos, visto que o desejado é tirar informações novas em cima de dados já conhecidos;
Estabilidade de clusterização, ou seja, diferentes ordens nos dados de inicialização devem levar à formação de clusters muito próximos como resultados;
Escalabilidade e performance, ou seja, um algoritmo de clusterização deve ser aplicável e utilizável em grandes bases de dados;
O formato dos clusters deve ser flexível, ou seja, é desejado que os clusters de dados não possuam formatos pré definidos pelo algoritmo.

	Para definição de qual algoritmo de clusterização utilizar, foi feito um estudo comparativo entre os mais usuais bem consolidados e testados. Dentre eles, o algoritmo mais vastamente utilizado é o k-means, por motivos de ser rápido, de fácil compreensão e fácil implementação. Entretanto, este algoritmo apresenta alguns problemas. O primeiro deles é o fato de não ser um algoritmo de clusterização, e sim um algoritmo de particionamento, ou seja, o algoritmo divide a base de dados em segmentos de formato globular, se opondo à boa prática de permitir clusters de formatos variáveis. Além disso, é necessário definir o número k de clusters que o algoritmo deve encontrar, limitando a quantidade de novos conhecimentos a ser explorada, e forçando que o algoritmo seja testado várias vezes, com diferente valores de k. A clusterização por meio do algoritmo de k-means também classifica todos os pontos de dado como pertencentes a algum cluster, assim como é sensível à ordem dos dados de inicialização. Com isso, o algoritmo de clusterização k-means não é aplicável como solução para a classificação de usuários de acordo com perfil de comportamento.
	Outro algoritmo analisado foi o mean shift. Este algoritmo consiste de uma técnica de análise espacial não paramétrica na qual busca-se localizar a máxima de uma função de densidade, de forma a, assim, retornar os clusters desejados. Ao contrário do k-means, não é necessário definir o número de clusters desejado para o algoritmo de mean shift. Apesar de este algoritmo não classificar todos os pontos de dado a algum cluster, os clusters ainda possuem formato globular, de pouca flexibilidade. Além disso, o algoritmo possui inicialização randômica, o que pode levar à baixa estabilidade de clusters.
	Por outro lado, o algoritmo de clusterização espectral faz uso dos autovalores e autovetores da matriz de similaridade dos dados de forma a performar redução de dimensionalidade antes, para clusterizar por meio de algum outro algoritmo, em menos dimensões. Dessa forma, a clusterização espectral pode ser considerada uma clusterização gráfica, na qual, em python, é utilizado o algoritmo de k-means como clusterizador. Com isso, os mesmos problemas de k-means são presentes na clusterização espectral.
	Os algoritmos de clusterização aglomerativa, por sua vez, tem por conceito iniciar com cada ponto de dado como um cluster e, então, fazer uso de algum critério para escolher outro cluster com quem fundí-lo. Isso ocorre repetidamente, até que seja formado um único cluster. A partir disso, obtém-se uma hierarquia, ou árvore binária, de clusters abrindo-se até a última camada, na qual há uma folha para cada ponto do dataset. Com isso, o formato dos clusters deixa de ser exclusivamente globular, mas persiste o problema de classificar todos os pontos de dados como pertencentes a algum cluster, além de, também, ser necessário escolher um valor k para o número de clusters desejado.
Por fim, o algoritmo de clusterização espacial baseada em densidade de aplicações com ruído, DBSCAN (Density-based spatial clustering of applications with noise), é um algoritmo de clusterização não paramétrico baseado em densidade, ou seja, no número de pontos de dados dentro de um certo raio específico, significativamente efetivo para identificar clusters de formatos arbitrários e de diferentes tamanhos, além de identificar e separar os ruídos dos dados, não classificando-os como pertencentes a cluster algum. Nele, são definidos: “core point” (ponto chave) como todo ponto de dado que possui um número mínimo de pontos, especificados pelo usuário, dentro do raio específico, “border point” (ponto de fronteira) como todo ponto localizado na vizinhança de um “core point”, ou seja, todo ponto cuja distância ao core point seja menos que o raio específico, e “noise point” (ponto de ruído) como todo ponto que não se classifica como core point ou border point.
O conceito por trás da construção de clusters baseados nas propriedades de densidade de uma base de dados baseia-se na própria estratégia humana de clusterização. Partindo deste princípio, ideia geral do DBSCAN é encontrar clusters com formatos arbitrários em bancos de dados espaciais que contenham ruídos, ideal para a aplicação em questão. O algoritmo DBSCAN consiste de: Arbitrariamente, seleciona-se um ponto p nos dados. A partir dele, identificam-se todos os pontos densamente conectados a p com relação aos parâmetros de raio específico e número mínimo de pontos. Caso p seja um core point, um cluster é formado. Caso p seja um border point e não hajam pontos conectados a p, DBSCAN visita o próximo ponto do conjunto de dados. O processo continua até que todos os pontos do conjunto de dados tenham sido analisados.
Uma das maiores vantagens do DBSCAN é a ausência de necessidade de especificar o número de clusters desejado, ao contrário de outros métodos como, por exemplo, k-means. Além disso, DBSCAN é capaz de encontrar clusters de formatos arbitrários, e também é capaz de lidar com dados outliers, não classificando-os como pertencentes a cluster algum, de forma estável, e também de boa performance. Por fim, o DBSCAN requer apenas dois parâmetros de entrada, que, por sua vez, são bastante intuitivos e de fácil manipulação durante a fase de testes, e é praticamente insensível à ordem dos pontos da base de dados.
Sucessor do DBSCAN, o HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) performa o DBSCAN com diferentes valores de raio específico, e integra o resultado de forma a encontrar a clusterização de melhor estabilidade. Em suma, o HDBSCAN estende o DBSCAN ao convertê-lo em um algoritmo de clusterização hierárquica, para depois usar de técnicas para extrair os clusters de maior estabilidade. Na prática, isso significa que o HDBSCAN retorna uma boa clusterização com relativa pouca afinação de parâmetros, e, o que é de extrema importância em modelos de clusterização, a partir de uma métrica intuitiva e fácil de selecionar, o tamanho mínimo dos clusters.

